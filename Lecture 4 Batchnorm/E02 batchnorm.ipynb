{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E02: BatchNorm, unlike other normalization layers like LayerNorm/GroupNorm etc. has the big advantage that after training, the batchnorm gamma/beta can be \"folded into\" the weights of the preceeding Linear layers, effectively erasing the need to forward it at test time. Set up a small 3-layer MLP with batchnorms, train the network, then \"fold\" the batchnorm gamma/beta into the preceeding Linear layer's W,b by creating a new W2, b2 and erasing the batch norm. Verify that this gives the same forward pass during inference. i.e. we see that the batchnorm is there just for stabilizing the training, and can be thrown out after training is done! pretty cool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names = open(\"../names.txt\", \"r\").read().splitlines()\n",
    "names[:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count the number of letters and prepare the index\n",
    "letters = set()\n",
    "for name in names:\n",
    "    for c in name:\n",
    "        letters.add(c)\n",
    "\n",
    "letter_list = list(letters)\n",
    "letter_list.append(\".\")\n",
    "letter_list.sort()\n",
    "\n",
    "stoi = {s:i for i, s in enumerate(letter_list)}\n",
    "itos = {i:s for s, i in stoi.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25626 28829\n",
      "torch.Size([182625, 3]) torch.Size([22655, 3]) torch.Size([22866, 3])\n"
     ]
    }
   ],
   "source": [
    "def prep_data(names, block_size):\n",
    "    x, y = [], []\n",
    "    for name in names:\n",
    "    # add the necessary padding to the name\n",
    "        modified_string = block_size * \".\" + name + \".\"\n",
    "        for ch1, ch2, ch3, ch4 in zip(modified_string, modified_string[1:], modified_string[2:], modified_string[3:]):\n",
    "            x.append([stoi[ch1], stoi[ch2], stoi[ch3]])\n",
    "            y.append(stoi[ch4])\n",
    "\n",
    "    X = torch.tensor(x)\n",
    "    Y = torch.tensor(y)\n",
    "\n",
    "    return X, Y\n",
    "\n",
    "# The data will be split as such - 80% training, 10% dev, 10% test\n",
    "BLOCK_SIZE = 3\n",
    "\n",
    "import random\n",
    "random.seed(42)\n",
    "random.shuffle(names)\n",
    "\n",
    "sample_size = len(names)\n",
    "train_size = int(0.8 * sample_size)\n",
    "dev_size = int(0.9 * sample_size)\n",
    "\n",
    "print(train_size, dev_size)\n",
    "\n",
    "Xtr, Ytr = prep_data(names[:train_size], BLOCK_SIZE)\n",
    "Xdev, Ydev = prep_data(names[train_size:dev_size], BLOCK_SIZE)\n",
    "Xtest, Ytest = prep_data(names[dev_size:], BLOCK_SIZE)\n",
    "\n",
    "print(Xtr.shape, Xdev.shape, Xtest.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torchify the code\n",
    "\n",
    "class Linear:\n",
    "    def __init__(self, fan_in, fan_out, gain=1, bias=True):\n",
    "        self.weight = torch.randn((fan_in, fan_out)) / (fan_in**0.5)\n",
    "        # seems like the biases should be torch.zeros?\n",
    "        self.bias = torch.zeros(fan_out) if bias else None\n",
    "\n",
    "    def __call__(self, input):\n",
    "        self.out = input @ self.weight\n",
    "        if self.bias != None:\n",
    "            self.out += self.bias\n",
    "        return self.out\n",
    "\n",
    "    def parameters(self):\n",
    "        return [self.weight] + ([self.bias] if self.bias != None else [])\n",
    "\n",
    "    def shape(self):\n",
    "        return self.weight.shape, self.bias.shape if self.bias != None else None\n",
    "\n",
    "class Tanh:    \n",
    "    def __call__(self, x):\n",
    "        self.out = torch.tanh(x)\n",
    "        return self.out\n",
    "\n",
    "    def parameters(self):\n",
    "        return []\n",
    "\n",
    "class BatchNorm1d:\n",
    "    def __init__(self, input_size, epsilon=1e-5, momentum=0.1):\n",
    "        self.epsilon = epsilon\n",
    "        self.momentum = momentum\n",
    "        # initialise the running mean and running var\n",
    "        self.training = True\n",
    "       \n",
    "        self.gamma = torch.ones(input_size)\n",
    "        self.beta = torch.zeros(input_size)\n",
    "\n",
    "        # for a normal distribution, the mean is 0 and the variance is 1\n",
    "        self.running_mean = torch.zeros(input_size)\n",
    "        self.running_var = torch.ones(input_size)\n",
    "\n",
    "    # normalize the values\n",
    "    def __call__(self, batch):\n",
    "        # # when dim=0, the output will be (100, 100) -> (100) -> this means that there is one average for each column=\n",
    "        # you only have to calculate the batch mean if you wanna update it\n",
    "        if self.training:\n",
    "            mean = batch.mean(0)\n",
    "            var = batch.var(0)\n",
    "\n",
    "        else:\n",
    "            mean = self.running_mean\n",
    "            var = self.running_var\n",
    "            \n",
    "        normalized = (batch - mean)/torch.sqrt(var + self.epsilon)\n",
    "        self.out = normalized * self.gamma + self.beta\n",
    "        \n",
    "        # update the running values if training\n",
    "        if self.training:\n",
    "            with torch.no_grad():\n",
    "                self.running_mean = self.running_mean * (1 - self.momentum) + mean * self.momentum\n",
    "                self.running_var = self.running_var * (1 - self.momentum) + var * self.momentum\n",
    "\n",
    "        return self.out\n",
    "\n",
    "    def parameters(self):\n",
    "        return [self.gamma] + [self.beta]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16694"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# hyperperameters are gonna be set here\n",
    "EMBEDDING_DIMENSION = 10\n",
    "HIDDEN_LAYER_SIZE = 100\n",
    "VOCAB_SIZE = 27\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647) # for reproducibility\n",
    "enc = torch.randn((VOCAB_SIZE, EMBEDDING_DIMENSION), generator=g)\n",
    "\n",
    "\n",
    "layers = [\n",
    "    Linear(EMBEDDING_DIMENSION * BLOCK_SIZE, HIDDEN_LAYER_SIZE, bias=False),\n",
    "    BatchNorm1d(HIDDEN_LAYER_SIZE, momentum=0.1),\n",
    "    Tanh(),\n",
    "    Linear(HIDDEN_LAYER_SIZE, HIDDEN_LAYER_SIZE, bias=False),\n",
    "    BatchNorm1d(HIDDEN_LAYER_SIZE, momentum=0.1),\n",
    "    Tanh(),\n",
    "    Linear(HIDDEN_LAYER_SIZE, VOCAB_SIZE, bias=False),\n",
    "    BatchNorm1d(VOCAB_SIZE, momentum=0.1),\n",
    "]\n",
    "\n",
    "# set the last layer to be less certain, so that the initial outputs are more uniform\n",
    "layers[-1].gamma *= 0.1\n",
    "\n",
    "# apply the right gain for batchnorm\n",
    "for layer in layers[:-1]:\n",
    "    if isinstance(layer, Linear):\n",
    "        layer.weight *= 0.7\n",
    "\n",
    "# count the number of layers and prepare for backprop\n",
    "parameters = [enc]\n",
    "\n",
    "for layer in layers:\n",
    "    parameters += layer.parameters()\n",
    "\n",
    "sum = enc.nelement()\n",
    "for p in parameters:\n",
    "    sum += p.nelement()\n",
    "    p.requires_grad = True\n",
    "\n",
    "sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The initial loss is 3.316511392593384\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(2.5477, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# track the updates and the data\n",
    "updates_to_data = []\n",
    "# perform training\n",
    "# perform a test run to check if the plotted graphs are eventually correct\n",
    "for i in range(1000):\n",
    "    random_sample = torch.randint(0, Xtr.shape[0], (32,), generator=g)\n",
    "    # create the encoding first\n",
    "    output = enc[Xtr[random_sample]].view(-1, BLOCK_SIZE * EMBEDDING_DIMENSION)\n",
    "\n",
    "    for layer in layers:\n",
    "        output = layer(output)\n",
    "\n",
    "    loss = F.cross_entropy(output, Ytr[random_sample])\n",
    "\n",
    "    if i == 0:\n",
    "        print(f'The initial loss is {loss}')\n",
    "\n",
    "    # for plotting\n",
    "    for layer in layers:\n",
    "        layer.out.retain_grad()\n",
    "\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    lr = 0.1 if i > 100000 else 0.01\n",
    "    for p in parameters:\n",
    "        p.data += -lr * p.grad\n",
    "\n",
    "    # what even are they tracking here man\n",
    "    with torch.no_grad():\n",
    "        updates_to_data.append([((lr*p.grad).std() / p.data.std()).log10().item() for p in parameters])\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.805344343185425"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calculate_loss(X, Y):\n",
    "    with torch.no_grad():\n",
    "        for layer in layers:\n",
    "            if isinstance(layer, BatchNorm1d):\n",
    "                layer.training = False\n",
    "            \n",
    "        xout = enc[X].view(-1, EMBEDDING_DIMENSION*BLOCK_SIZE)\n",
    "        for layer in layers:\n",
    "            xout = layer(xout)\n",
    "\n",
    "        loss = F.cross_entropy(xout, Y)\n",
    "\n",
    "    return loss.item()\n",
    "calculate_loss(Xdev, Ydev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# layers = [\n",
    "#     Linear(EMBEDDING_DIMENSION * BLOCK_SIZE, HIDDEN_LAYER_SIZE, bias=False),\n",
    "#     BatchNorm1d(HIDDEN_LAYER_SIZE, momentum=0.1),\n",
    "#     Tanh(),\n",
    "#     Linear(HIDDEN_LAYER_SIZE, HIDDEN_LAYER_SIZE, bias=False),\n",
    "#     BatchNorm1d(HIDDEN_LAYER_SIZE, momentum=0.1),\n",
    "#     Tanh(),\n",
    "#     Linear(HIDDEN_LAYER_SIZE, VOCAB_SIZE, bias=False),\n",
    "#     BatchNorm1d(VOCAB_SIZE, momentum=0.1),\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-0.0004,  0.0451,  0.0592,  ..., -0.0636, -0.0807, -0.1227],\n",
       "         [ 0.0448, -0.0233,  0.0886,  ..., -0.1760, -0.1986, -0.1342],\n",
       "         [ 0.1465,  0.1298, -0.0687,  ..., -0.1589,  0.0764,  0.0351],\n",
       "         ...,\n",
       "         [-0.0287, -0.0839,  0.0246,  ...,  0.0157,  0.2501,  0.0295],\n",
       "         [ 0.1064, -0.2199,  0.1551,  ..., -0.0513, -0.0155,  0.0283],\n",
       "         [ 0.2185,  0.2005, -0.0183,  ...,  0.1906, -0.1868,  0.0493]],\n",
       "        grad_fn=<MulBackward0>),\n",
       " tensor([ 9.6443e-04, -2.0612e-03, -3.2228e-03, -2.9549e-03,  8.0622e-04,\n",
       "         -5.8683e-03,  1.2179e-03,  7.2280e-04,  1.0903e-03, -5.2481e-03,\n",
       "         -9.2608e-04,  3.8336e-04, -3.6237e-03, -3.6296e-03,  2.6891e-03,\n",
       "          2.0360e-03,  2.2279e-04, -1.4937e-03, -4.2076e-03, -1.3299e-04,\n",
       "          2.3582e-03,  1.7673e-03, -3.5823e-03,  1.1244e-03,  2.0174e-03,\n",
       "         -1.7253e-03,  3.8006e-04, -9.1260e-03,  1.3912e-03,  2.2438e-03,\n",
       "          1.9858e-03, -2.0389e-03, -2.5857e-04,  5.5556e-03,  8.0743e-03,\n",
       "          9.9651e-04,  1.4092e-03,  2.4981e-03, -5.4595e-04, -1.1811e-03,\n",
       "          2.2140e-03,  3.1012e-04,  4.2688e-03, -4.5829e-03, -8.2694e-04,\n",
       "         -1.6473e-03, -3.7460e-04,  1.4799e-03, -3.3290e-04, -9.4386e-04,\n",
       "          2.1805e-03, -8.5283e-05, -2.2841e-03, -1.0553e-03,  4.0290e-03,\n",
       "         -7.4647e-04,  4.2724e-07, -1.0533e-03, -1.9022e-03,  2.2471e-03,\n",
       "          5.4494e-05,  4.4443e-04, -9.4530e-04, -9.3954e-04,  4.0233e-04,\n",
       "          6.7817e-04,  2.4150e-03,  1.6226e-03, -1.1110e-03, -5.5247e-03,\n",
       "         -1.9187e-03, -5.0635e-04, -1.8600e-03,  3.6501e-04, -5.2026e-03,\n",
       "         -1.9496e-04,  9.2134e-04,  6.4317e-04, -2.6966e-03,  2.2811e-03,\n",
       "         -3.5956e-03, -2.1068e-03, -2.7273e-03,  3.3193e-03, -4.7537e-05,\n",
       "          1.1153e-03, -5.7035e-04, -2.0815e-04, -9.8871e-03, -3.5470e-03,\n",
       "          1.7709e-03,  2.4403e-04,  3.6744e-04, -2.3870e-03, -9.5122e-05,\n",
       "          7.7974e-05,  4.9910e-04, -3.5591e-04,  1.5040e-04, -1.2724e-03],\n",
       "        requires_grad=True))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# my assumption is that 'folding the weights' means multiplying the weights by that of the previous batch\n",
    "w2 = layers[0].weight * layers[1].gamma\n",
    "b2 = layers[1].beta\n",
    "\n",
    "w2, b2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<__main__.Layer at 0x13f45d480>,\n",
       " <__main__.Tanh at 0x14a371600>,\n",
       " <__main__.Layer at 0x14a24db70>,\n",
       " <__main__.Tanh at 0x13f45f190>,\n",
       " <__main__.Layer at 0x13f402e00>,\n",
       " <__main__.Tanh at 0x14a2013f0>]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Layer:\n",
    "    def __init__(self, weight, bias):\n",
    "        self.weight = weight\n",
    "        self.bias = bias\n",
    "\n",
    "    def __call__(self, x):\n",
    "        self.out = x @ self.weight + self.bias\n",
    "        return self.out\n",
    "\n",
    "new_layers = []\n",
    "for i in range(0, len(layers), 3):\n",
    "    w2 = layers[i].weight * layers[i + 1].gamma\n",
    "    # my Linear layers in the layers list does not have any bias \n",
    "    b2 = layers[i + 1].beta\n",
    "\n",
    "    new_layer = Layer(w2, b2)\n",
    "    new_layers.append(new_layer)\n",
    "    new_layers.append(Tanh())\n",
    "\n",
    "new_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2.805344343185425, 2.805344343185425, True)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calculate_loss_folded(X, Y):\n",
    "    with torch.no_grad():\n",
    "        xout = enc[X].view(-1, EMBEDDING_DIMENSION*BLOCK_SIZE)\n",
    "        for layer in layers:\n",
    "            xout = layer(xout)\n",
    "\n",
    "        loss = F.cross_entropy(xout, Y)\n",
    "\n",
    "    return loss.item()\n",
    "\n",
    "# verify that it was done correctly\n",
    "calculate_loss_folded(Xdev, Ydev), calculate_loss(Xdev, Ydev), calculate_loss_folded(Xdev, Ydev) == calculate_loss(Xdev, Ydev)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gp-gpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

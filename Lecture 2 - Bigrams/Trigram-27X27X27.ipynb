{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = open(\"names.txt\", \"r\").read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect a list of all the letters\n",
    "name_set = set()\n",
    "for name in names:\n",
    "    for c in name:\n",
    "        name_set.add(c)\n",
    "\n",
    "char_list = list(name_set)\n",
    "char_list.append(\".\")\n",
    "sorted_letter = sorted(char_list)\n",
    "\n",
    "# create a dictionary that contains the mapping of the index to the character\n",
    "stoi = {s:i for i, s in enumerate(sorted_letter)}\n",
    "itos = {i:s for s, i in stoi.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "196113"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set out the letter counting\n",
    "# i do not get how this can be visualised though...\n",
    "# counts = torch.zeros((27, 27, 27))\n",
    "\n",
    "# input this will take a tuple\n",
    "x = []\n",
    "\n",
    "# output\n",
    "y = []\n",
    "\n",
    "for word in names:\n",
    "    formatted_word = \".\" + word + \".\"\n",
    "    for c1, c2, c3 in zip(formatted_word, formatted_word[1:], formatted_word[2:]):\n",
    "        x.append((stoi[c1], stoi[c2]))\n",
    "        y.append(stoi[c3])\n",
    "\n",
    "x = torch.tensor(x)\n",
    "y = torch.tensor(y)\n",
    "\n",
    "x.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([196113, 729])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_tensors = []\n",
    "\n",
    "# try making my own variation of ohe that can represent 2 inputs\n",
    "for count, v in enumerate(x):\n",
    "    base = torch.zeros((27, 27))\n",
    "    # everything that starts with '.' will be in the first row. anything that ends with '.' will be in the first column\n",
    "    base[v[0], v[1]] += 1\n",
    "    # print(base.shape)\n",
    "    input_tensors.append(base)\n",
    "\n",
    "xenc = torch.stack(input_tensors, dim=0)\n",
    "xenc.shape\n",
    "\n",
    "# reshape the tensor for multiplication -> 3 states the number of rows, while -1 means that the column dimension can be inferred after calculation\n",
    "xenc = xenc.view(x.shape[0], -1)\n",
    "xenc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 0., 1., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 1., 0., 0.]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xenc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([729, 27])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now that all the data has been prepared, it is time to make the neural network\n",
    "\n",
    "# start by creating weights from the seed\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "W = torch.randn((729, 27), generator=g, requires_grad=True)\n",
    "W.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 3.7925939559936523\n",
      "1: 3.712592124938965\n",
      "2: 3.6333956718444824\n",
      "3: 3.5550966262817383\n",
      "4: 3.477797269821167\n",
      "5: 3.4016079902648926\n",
      "6: 3.3266489505767822\n",
      "7: 3.2530415058135986\n",
      "8: 3.1809046268463135\n",
      "9: 3.110346794128418\n",
      "10: 3.0414631366729736\n",
      "11: 2.9743270874023438\n",
      "12: 2.9089927673339844\n",
      "13: 2.8454933166503906\n",
      "14: 2.783843994140625\n",
      "15: 2.724045991897583\n",
      "16: 2.6660871505737305\n",
      "17: 2.609947681427002\n",
      "18: 2.555598258972168\n",
      "19: 2.503004550933838\n",
      "20: 2.452125072479248\n",
      "21: 2.402916669845581\n",
      "22: 2.355332136154175\n",
      "23: 2.3093228340148926\n",
      "24: 2.2648396492004395\n",
      "25: 2.2218334674835205\n",
      "26: 2.1802544593811035\n",
      "27: 2.1400551795959473\n",
      "28: 2.1011877059936523\n",
      "29: 2.063605785369873\n",
      "30: 2.027263641357422\n",
      "31: 1.9921172857284546\n",
      "32: 1.9581228494644165\n",
      "33: 1.9252378940582275\n",
      "34: 1.8934203386306763\n",
      "35: 1.862629771232605\n",
      "36: 1.8328256607055664\n",
      "37: 1.803969144821167\n",
      "38: 1.7760224342346191\n",
      "39: 1.7489479780197144\n",
      "40: 1.722710132598877\n",
      "41: 1.6972737312316895\n",
      "42: 1.6726058721542358\n",
      "43: 1.6486736536026\n",
      "44: 1.6254464387893677\n",
      "45: 1.6028944253921509\n",
      "46: 1.580989122390747\n",
      "47: 1.5597034692764282\n",
      "48: 1.539011836051941\n",
      "49: 1.5188889503479004\n",
      "50: 1.4993118047714233\n",
      "51: 1.4802579879760742\n",
      "52: 1.4617063999176025\n",
      "53: 1.443636417388916\n",
      "54: 1.4260294437408447\n",
      "55: 1.4088666439056396\n",
      "56: 1.3921310901641846\n",
      "57: 1.3758059740066528\n",
      "58: 1.3598755598068237\n",
      "59: 1.3443251848220825\n",
      "60: 1.329140543937683\n",
      "61: 1.3143078088760376\n",
      "62: 1.2998145818710327\n",
      "63: 1.2856481075286865\n",
      "64: 1.2717969417572021\n",
      "65: 1.2582497596740723\n",
      "66: 1.2449958324432373\n",
      "67: 1.2320252656936646\n",
      "68: 1.2193282842636108\n",
      "69: 1.2068957090377808\n",
      "70: 1.194718360900879\n",
      "71: 1.1827882528305054\n",
      "72: 1.1710971593856812\n",
      "73: 1.159637212753296\n",
      "74: 1.1484012603759766\n",
      "75: 1.1373820304870605\n",
      "76: 1.126573085784912\n",
      "77: 1.1159676313400269\n",
      "78: 1.1055599451065063\n",
      "79: 1.095343828201294\n",
      "80: 1.0853135585784912\n",
      "81: 1.0754637718200684\n",
      "82: 1.0657895803451538\n",
      "83: 1.0562856197357178\n",
      "84: 1.0469472408294678\n",
      "85: 1.0377697944641113\n",
      "86: 1.0287491083145142\n",
      "87: 1.019881010055542\n",
      "88: 1.011160969734192\n",
      "89: 1.002585768699646\n",
      "90: 0.9941511154174805\n",
      "91: 0.9858536720275879\n",
      "92: 0.9776899218559265\n",
      "93: 0.9696565270423889\n",
      "94: 0.9617504477500916\n",
      "95: 0.9539684057235718\n",
      "96: 0.9463074207305908\n",
      "97: 0.9387650489807129\n",
      "98: 0.9313380718231201\n",
      "99: 0.9240241050720215\n",
      "100: 0.9168202877044678\n",
      "101: 0.9097245335578918\n",
      "102: 0.9027339816093445\n",
      "103: 0.8958467841148376\n",
      "104: 0.8890606164932251\n",
      "105: 0.8823729753494263\n",
      "106: 0.8757822513580322\n",
      "107: 0.8692860007286072\n",
      "108: 0.8628824353218079\n",
      "109: 0.8565696477890015\n",
      "110: 0.8503458499908447\n",
      "111: 0.8442092537879944\n",
      "112: 0.8381578922271729\n",
      "113: 0.8321903944015503\n",
      "114: 0.8263048529624939\n",
      "115: 0.8204999566078186\n",
      "116: 0.8147738575935364\n",
      "117: 0.8091251850128174\n",
      "118: 0.8035524487495422\n",
      "119: 0.7980542182922363\n",
      "120: 0.7926291823387146\n",
      "121: 0.7872758507728577\n",
      "122: 0.7819928526878357\n",
      "123: 0.776779055595398\n",
      "124: 0.7716330885887146\n",
      "125: 0.7665538191795349\n",
      "126: 0.761539876461029\n",
      "127: 0.7565902471542358\n",
      "128: 0.7517037987709045\n",
      "129: 0.7468792200088501\n",
      "130: 0.7421154975891113\n",
      "131: 0.7374116778373718\n",
      "132: 0.7327665686607361\n",
      "133: 0.7281790971755981\n",
      "134: 0.7236484289169312\n",
      "135: 0.7191734910011292\n",
      "136: 0.7147532105445862\n",
      "137: 0.7103867530822754\n",
      "138: 0.7060732245445251\n",
      "139: 0.7018117308616638\n",
      "140: 0.697601318359375\n",
      "141: 0.6934411525726318\n",
      "142: 0.6893303394317627\n",
      "143: 0.6852681636810303\n",
      "144: 0.6812536120414734\n",
      "145: 0.6772860288619995\n",
      "146: 0.6733646392822266\n",
      "147: 0.6694886088371277\n",
      "148: 0.6656572818756104\n",
      "149: 0.6618697047233582\n",
      "150: 0.6581254005432129\n",
      "151: 0.6544235944747925\n",
      "152: 0.6507635116577148\n",
      "153: 0.6471444964408875\n",
      "154: 0.6435660123825073\n",
      "155: 0.6400272250175476\n",
      "156: 0.6365275382995605\n",
      "157: 0.6330663561820984\n",
      "158: 0.6296430230140686\n",
      "159: 0.626257061958313\n",
      "160: 0.6229077577590942\n",
      "161: 0.6195946335792542\n",
      "162: 0.6163169741630554\n",
      "163: 0.6130743026733398\n",
      "164: 0.6098660230636597\n",
      "165: 0.6066916584968567\n",
      "166: 0.6035507321357727\n",
      "167: 0.6004425883293152\n",
      "168: 0.5973667502403259\n",
      "169: 0.5943228602409363\n",
      "170: 0.5913102626800537\n",
      "171: 0.5883285999298096\n",
      "172: 0.5853773355484009\n",
      "173: 0.5824560523033142\n",
      "174: 0.5795642137527466\n",
      "175: 0.5767014622688293\n",
      "176: 0.5738673806190491\n",
      "177: 0.5710614323616028\n",
      "178: 0.5682832598686218\n",
      "179: 0.5655325055122375\n",
      "180: 0.5628086924552917\n",
      "181: 0.560111403465271\n",
      "182: 0.5574404001235962\n",
      "183: 0.5547952055931091\n",
      "184: 0.5521753430366516\n",
      "185: 0.5495806336402893\n",
      "186: 0.5470105409622192\n",
      "187: 0.5444648861885071\n",
      "188: 0.5419431924819946\n",
      "189: 0.5394452214241028\n",
      "190: 0.5369704961776733\n",
      "191: 0.534518837928772\n",
      "192: 0.5320898294448853\n",
      "193: 0.5296832323074341\n",
      "194: 0.527298629283905\n",
      "195: 0.524935781955719\n",
      "196: 0.5225944519042969\n",
      "197: 0.5202741622924805\n",
      "198: 0.5179747939109802\n",
      "199: 0.5156960487365723\n"
     ]
    }
   ],
   "source": [
    "for i in range(200):\n",
    "    logits = xenc @ W\n",
    "    exp = logits.exp()\n",
    "    sum = exp.sum(dim=1, keepdim=True)\n",
    "    probs = exp / sum\n",
    "\n",
    "    loss = -probs[torch.arange(xenc.shape[0]), y[0]].log().mean()\n",
    "    print(f\"{i}: {loss}\")\n",
    "\n",
    "    W.grad = None\n",
    "    loss.backward()\n",
    "\n",
    "    W.data += -10 * W.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vmexzdfzjglkurmimczkwyhhmvlzimmtnammmlbfvk.\n",
      "mmammammwmxqpuhptthmmimmwxezgzjedmnvugkwptedmwekjemmmmsammy.\n",
      "amkxvjrnfrqtbspmhwcjdemmtrmxuow.\n",
      "wmsfxxblgjxlhgfiwuipgnammwfdnimcwzktsdemm.\n",
      "zmommt.\n"
     ]
    }
   ],
   "source": [
    "# now we cna try to sample from the trigram model. LOL\n",
    "\n",
    "import random\n",
    "# random.seed(2147483647)\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "\n",
    "for i in range(5):\n",
    "    # for the first, choose the second letter as a random letter\n",
    "    first_index = 0\n",
    "    second_index = random.randint(1, 26)\n",
    "    output = [itos[second_index]]\n",
    "\n",
    "    while True:\n",
    "        # create a one hot encoding for the first two letters\n",
    "        base = torch.zeros((27, 27))\n",
    "        base[first_index, second_index] += 1\n",
    "        # squash it into a one-hot encoding that can be fed into the NN\n",
    "        xenc = base.view(1, -1) # this would probably be 1, 729\n",
    "\n",
    "        logits = xenc @ W\n",
    "        probs = logits.exp()\n",
    "        P = probs / probs.sum(dim=1, keepdim=True)\n",
    "        output_index = torch.multinomial(P, num_samples=1, replacement=True, generator=g).item()\n",
    "\n",
    "        output.append(itos[output_index])\n",
    "        first_index = second_index\n",
    "        second_index = output_index\n",
    "\n",
    "        if output_index == 0:\n",
    "            break\n",
    "\n",
    "    print(\"\".join(output))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#########\n",
    "Below are some of the intermediate working cells\n",
    "########"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (torch)",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = open(\"names.txt\", \"r\").read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect a list of all the letters\n",
    "name_set = set()\n",
    "for name in names:\n",
    "    for c in name:\n",
    "        name_set.add(c)\n",
    "\n",
    "char_list = list(name_set)\n",
    "char_list.append(\".\")\n",
    "sorted_letter = sorted(char_list)\n",
    "\n",
    "# create a dictionary that contains the mapping of the index to the character\n",
    "stoi = {s:i for i, s in enumerate(sorted_letter)}\n",
    "itos = {i:s for s, i in stoi.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "196113"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set out the letter counting\n",
    "# i do not get how this can be visualised though...\n",
    "# counts = torch.zeros((27, 27, 27))\n",
    "\n",
    "# input this will take a tuple\n",
    "x = []\n",
    "\n",
    "# output\n",
    "y = []\n",
    "\n",
    "for word in names:\n",
    "    formatted_word = \".\" + word + \".\"\n",
    "    for c1, c2, c3 in zip(formatted_word, formatted_word[1:], formatted_word[2:]):\n",
    "        x.append((stoi[c1], stoi[c2]))\n",
    "        y.append(stoi[c3])\n",
    "\n",
    "x = torch.tensor(x)\n",
    "y = torch.tensor(y)\n",
    "\n",
    "x.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([196113, 729])"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_tensors = []\n",
    "\n",
    "# try making my own variation of ohe that can represent 2 inputs\n",
    "for count, v in enumerate(x):\n",
    "    base = torch.zeros((27, 27))\n",
    "    # everything that starts with '.' will be in the first row. anything that ends with '.' will be in the first column\n",
    "    base[v[0], v[1]] += 1\n",
    "    # print(base.shape)\n",
    "    input_tensors.append(base)\n",
    "\n",
    "xenc = torch.stack(input_tensors, dim=0)\n",
    "xenc.shape\n",
    "\n",
    "# reshape the tensor for multiplication -> 3 states the number of rows, while -1 means that the column dimension can be inferred after calculation\n",
    "xenc = xenc.view(x.shape[0], -1)\n",
    "xenc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([729, 27])"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now that all the data has been prepared, it is time to make the neural network\n",
    "\n",
    "# start by creating weights from the seed\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "W = torch.randn((729, 27), generator=g, requires_grad=True)\n",
    "W.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5000):\n",
    "    logits = xenc @ W\n",
    "    exp = logits.exp()\n",
    "    sum = exp.sum(dim=1, keepdim=True)\n",
    "    probs = exp / sum\n",
    "\n",
    "    loss = -probs[torch.arange(xenc.shape[0]), y].log().mean()\n",
    "    print(f\"{i}: {loss}\")\n",
    "\n",
    "    W.grad = None\n",
    "    loss.backward()\n",
    "\n",
    "    W.data += -5 * W.grad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.1570632457733154"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ocexzdfzjglkuriana.\n",
      "ha.\n",
      "yah.\n",
      "her.\n",
      "olistona.\n"
     ]
    }
   ],
   "source": [
    "# now we cna try to sample from the trigram model. LOL\n",
    "\n",
    "import random\n",
    "# random.seed(2147483647)\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "\n",
    "for i in range(5):\n",
    "    # for the first, choose the second letter as a random letter\n",
    "    first_index = 0\n",
    "    second_index = random.randint(1, 26)\n",
    "    output = [itos[second_index]]\n",
    "\n",
    "    while True:\n",
    "        # create a one hot encoding for the first two letters\n",
    "        base = torch.zeros((27, 27))\n",
    "        base[first_index, second_index] += 1\n",
    "        # squash it into a one-hot encoding that can be fed into the NN\n",
    "        xenc = base.view(1, -1) # this would probably be 1, 729\n",
    "\n",
    "        logits = xenc @ W\n",
    "        probs = logits.exp()\n",
    "        P = probs / probs.sum(dim=1, keepdim=True)\n",
    "        output_index = torch.multinomial(P, num_samples=1, replacement=True, generator=g).item()\n",
    "\n",
    "        output.append(itos[output_index])\n",
    "        first_index = second_index\n",
    "        second_index = output_index\n",
    "\n",
    "        if output_index == 0:\n",
    "            break\n",
    "\n",
    "    print(\"\".join(output))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E04: we saw that our 1-hot vectors merely select a row of W, so producing these vectors explicitly feels wasteful. \n",
    "Can you delete our use of F.one_hot in favor of simply indexing into rows of W?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.2761,  1.0152,  0.1471,  0.7295,  1.4850,  0.2727, -0.7242,  0.3931,\n",
      "        -0.6920,  0.3070,  0.9341,  0.0767, -0.5380,  5.9423, -0.5895,  0.3441,\n",
      "         0.2779, -1.4199,  1.1005,  0.0231,  0.6784,  0.3041,  0.7013, -1.9261,\n",
      "        -0.5120, -2.1419,  0.6578], grad_fn=<SliceBackward0>)\n",
      "tensor([[ 0.2761,  1.0152,  0.1471,  0.7295,  1.4850,  0.2727, -0.7242,  0.3931,\n",
      "         -0.6920,  0.3070,  0.9341,  0.0767, -0.5380,  5.9423, -0.5895,  0.3441,\n",
      "          0.2779, -1.4199,  1.1005,  0.0231,  0.6784,  0.3041,  0.7013, -1.9261,\n",
      "         -0.5120, -2.1419,  0.6578]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "W.shape # torch.Size([729, 27])\n",
    "\n",
    "test = x[0] #(0, 5)\n",
    "\n",
    "# I think that figuring out which row to index into is as such\n",
    "index = test[0] * 27 + test[1]\n",
    "\n",
    "print(W[index, :])\n",
    "\n",
    "# trying it with one hot encoding\n",
    "base = torch.zeros((27, 27))\n",
    "base[test[0], test[1]] += 1\n",
    "xenc_t = base.view(1, -1)\n",
    "\n",
    "print(xenc_t @ W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E05: look up and use F.cross_entropy instead. You should achieve the same result. \n",
    "\n",
    "Can you think of why we'd prefer to use F.cross_entropy instead?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "# when calculating cross entropy, you are required to provide (a) unnormalized logits for each class (b) target, which in this case should be expressed as OHEs\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "\n",
    "# reset the weights\n",
    "Wn = torch.randn((729, 27), requires_grad=True, generator=g).float()\n",
    "\n",
    "# calculate the result \n",
    "logits = xenc @ Wn\n",
    "\n",
    "# calculate the targets' OHE\n",
    "yenc = F.one_hot(y, num_classes=27).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.equal(W, Wn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.7231, grad_fn=<DivBackward1>)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = F.cross_entropy(logits, yenc)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 3.723123073577881\n"
     ]
    }
   ],
   "source": [
    "for i in range(1):\n",
    "    logits = xenc @ Wn\n",
    "    loss = F.cross_entropy(logits, yenc)\n",
    "    print(f\"{i}: {loss}\")\n",
    "\n",
    "    Wn.grad = None\n",
    "    loss.backward()\n",
    "\n",
    "    Wn.data += -1 * W.grad"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (torch)",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

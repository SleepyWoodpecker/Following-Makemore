{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- E01: Tune the hyperparameters of the training to beat my best validation loss of 2.2\n",
    "\n",
    "\n",
    "- E02: I was not careful with the intialization of the network in this video. \n",
    "\n",
    "(1) What is the loss you'd get if the predicted probabilities at initialization were perfectly uniform? What loss do we achieve? \n",
    "\n",
    "(2) Can you tune the initialization to get a starting loss that is much more similar to (1)?\n",
    "\n",
    "\n",
    "- E03: Read the Bengio et al 2003 paper (link above), implement and try any idea from the paper. Did it work?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['emma', 'olivia', 'ava']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  load the names\n",
    "names = open(\"../names.txt\", \"r\").read().splitlines()\n",
    "names[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count the number of letters and prepare the index\n",
    "letters = set()\n",
    "for name in names:\n",
    "    for c in name:\n",
    "        letters.add(c)\n",
    "\n",
    "letter_list = list(letters)\n",
    "letter_list.append(\".\")\n",
    "letter_list.sort()\n",
    "\n",
    "stoi = {s:i for i, s in enumerate(letter_list)}\n",
    "itos = {i:s for s, i in stoi.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the dataset\n",
    "# the number of letters you take in as context should be the amount of padding you place at the start\n",
    "\n",
    "# this will be the inputs\n",
    "x = []\n",
    "\n",
    "# this will be the outputs\n",
    "y = []\n",
    "\n",
    "# padding size\n",
    "block_size = 3\n",
    "\n",
    "for name in names[:50]:\n",
    "    # add the necessary padding to the name\n",
    "    modified_string = block_size * \".\" + name + \".\"\n",
    "    for ch1, ch2, ch3, ch4 in zip(modified_string, modified_string[1:], modified_string[2:], modified_string[3:]):\n",
    "        x.append([stoi[ch1], stoi[ch2], stoi[ch3]])\n",
    "        y.append(stoi[ch4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([337, 3])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert X and Y into tensors\n",
    "X = torch.tensor(x)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([337])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y = torch.tensor(y)\n",
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set a random seed for the generator\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "\n",
    "# create an encoding format for the model\n",
    "# start with a 27, 2 -> each character is represented by a 2 dimension vector\n",
    "enc = torch.randn((27, 2), generator=g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 3, 2])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# note to self from video! Indexing can be done with tensors as well, so this is super convenient! :D\n",
    "enc[X[:3]].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the first hidden layer for the network\n",
    "# (number of inputs that should be taken, number of outputs that should be produced)\n",
    "\n",
    "# this is the first layer\n",
    "w1 = torch.randn((3*2, 100), generator=g)\n",
    "b1 = torch.randn(100, generator=g)\n",
    "\n",
    "# this is the output layer\n",
    "w2 = torch.randn((100, 27), generator=g)\n",
    "b2 = torch.randn(27, generator=g)\n",
    "\n",
    "# store the params in a list to make life easier later\n",
    "params = [enc, w1, b1, w2, b2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3481\n"
     ]
    }
   ],
   "source": [
    "# check the number of parameters\n",
    "print(sum([p.nelement() for p in params]))\n",
    "\n",
    "\n",
    "for p in params:\n",
    "    p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(19.4104, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "# time to construct the forward pass\n",
    "\n",
    "# create the encodings for all of the inputs and feed them into the network\n",
    "\n",
    "# actually kind of unsure why the output here is tanh-ed\n",
    "input = enc[X].view(X.shape[0], -1)\n",
    "h1 = torch.tanh(input @ w1 + b1)\n",
    "\n",
    "# pass it to the second layer\n",
    "output = h1 @ w2 + b2\n",
    "\n",
    "# calculate the loss of the current NN\n",
    "loss = F.cross_entropy(output, Y)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(14.5042, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# testing if backpropagation can occur\n",
    "for p in params:\n",
    "    p.grad = None\n",
    "\n",
    "loss.backward()\n",
    "\n",
    "for p in params:\n",
    "    p.data += -0.1 * p.grad\n",
    "\n",
    "# actually kind of unsure why the output here is tanh-ed\n",
    "input = enc[X].view(X.shape[0], -1)\n",
    "h1 = torch.tanh(input @ w1 + b1)\n",
    "\n",
    "# pass it to the second layer\n",
    "output = h1 @ w2 + b2\n",
    "\n",
    "# calculate the loss of the current NN\n",
    "loss = F.cross_entropy(output, Y)\n",
    "\n",
    "loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (torch)",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

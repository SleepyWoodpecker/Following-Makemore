{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "# time to construct the forward pass\n",
    "\n",
    "# create the encodings for all of the inputs and feed them into the network\n",
    "\n",
    "# actually kind of unsure why the output here is tanh-ed\n",
    "input = enc[X].view(X.shape[0], -1)\n",
    "h1 = torch.tanh(input @ w1 + b1)\n",
    "\n",
    "# pass it to the second layer\n",
    "output = h1 @ w2 + b2\n",
    "\n",
    "# calculate the loss of the current NN\n",
    "loss = F.cross_entropy(output, Y)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing if backpropagation can occur\n",
    "for p in params:\n",
    "    p.grad = None\n",
    "\n",
    "loss.backward()\n",
    "\n",
    "for p in params:\n",
    "    p.data += -0.1 * p.grad\n",
    "\n",
    "# actually kind of unsure why the output here is tanh-ed\n",
    "input = enc[X].view(X.shape[0], -1)\n",
    "h1 = torch.tanh(input @ w1 + b1)\n",
    "\n",
    "# pass it to the second layer\n",
    "output = h1 @ w2 + b2\n",
    "\n",
    "# calculate the loss of the current NN\n",
    "loss = F.cross_entropy(output, Y)\n",
    "\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random_sample = torch.randint(0, X.shape[0], (3,), generator=g)\n",
    "print(X[[1, 2, 3, 4]])\n",
    "print(Y[[1, 2, 3]])\n",
    "\n",
    "# note to self from video! Indexing can be done with tensors as well, so this is super convenient! :D\n",
    "enc[X[:3]].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Somehow, removing b2 causes the NN to stop learning? When b2 was part of the computation though, the NN actually managed to learn, which I do not understand why?\n",
    "for i in range(10):\n",
    "    encodings = enc[X]\n",
    "    o1 = torch.tanh(encodings.view(X.shape[0], -1) @ w1 + b1)\n",
    "    output = o1 @ w2 + b2\n",
    "    loss = torch.nn.functional.cross_entropy(output, Y)\n",
    "\n",
    "    print(loss.item())\n",
    "\n",
    "    for p in params:\n",
    "        p.grad = None\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    for p in params:\n",
    "        p.data += -0.1 * p.grad\n",
    "\n",
    "    print(b2.grad)    \n",
    "\n",
    "\n",
    "# checking what has changed\n",
    "\n",
    "w1, b1, w2, b2\n",
    "output\n",
    "When initialized with constant weights and biases, it seems that the output probabilities do not change regardless of the input. Since the probabilities are all equal at the start, the learning rate is the same at the start, regardless of what the output is. "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

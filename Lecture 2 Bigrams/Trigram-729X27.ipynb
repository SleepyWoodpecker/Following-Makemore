{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "85e860d5-b29f-46b5-882a-e31914eea6a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e8f6da3-2c17-402b-aca1-38404dbda788",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tasks\n",
    "# E01: train a trigram language model, i.e. take two characters as an input to predict the 3rd one. \n",
    "# Feel free to use either counting or a neural net. Evaluate the loss; Did it improve over a bigram model? \n",
    "# -> i will try to make this implementation using a neural net\n",
    "\n",
    "# E02: split up the dataset randomly into 80% train set, 10% dev set, 10% test set. \n",
    "# Train the bigram and trigram models only on the training set. Evaluate them on dev and test splits. What can you see?\n",
    "\n",
    "# E03: use the dev set to tune the strength of smoothing (or regularization) for the trigram model \n",
    "# - i.e. try many possibilities and see which one works best based on the dev set loss. What patterns can you see in the train and dev set loss as you tune this strength? Take the best setting of the smoothing and evaluate on the test set once and at the end. \n",
    "# How good of a loss do you achieve?\n",
    "\n",
    "# E04: we saw that our 1-hot vectors merely select a row of W, so producing these vectors explicitly feels wasteful. \n",
    "# Can you delete our use of F.one_hot in favor of simply indexing into rows of W?\n",
    "\n",
    "# E05: look up and use F.cross_entropy instead. You should achieve the same result. \n",
    "# Can you think of why we'd prefer to use F.cross_entropy instead?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c445ebc-eb9a-456c-9f08-4c069d61d6b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "names = open(\"names.txt\", \"r\").read().splitlines()\n",
    "# names[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "db6c4f01-44d5-441c-a174-82b836580275",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a set of characters to integers\n",
    "all_letters = set()\n",
    "for name in names:\n",
    "    for char in name:\n",
    "        all_letters.add(char)\n",
    "\n",
    "all_letters = list(all_letters)\n",
    "all_letters.append(\".\")\n",
    "sorted_letters = sorted(all_letters)\n",
    "\n",
    "# this is going ot be used for the output layer\n",
    "stoi = {l:count for count, l in enumerate(sorted_letters)} #string to integer\n",
    "stoi[\".\"] = 0\n",
    "\n",
    "# this is going to be used for the input layer\n",
    "# now this starts at 0 as well\n",
    "btoi = {}\n",
    "for outc, ch1 in (enumerate(sorted_letters)):\n",
    "    for inc, ch2 in enumerate(sorted_letters):\n",
    "        index = outc * 27 + inc\n",
    "        btoi[ch1+ch2] = index\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "20bd52fc-6220-419f-8452-600c3dec95be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this will be the inputs\n",
    "x = []\n",
    "\n",
    "# this will be the outputs\n",
    "y = []\n",
    "\n",
    "# # chars\n",
    "# chars = []\n",
    "\n",
    "# create trigrams from the names, add a \".\" to the front and the back. Trigrams use the first 2 letters to predict the third\n",
    "for name in names[:20]:\n",
    "    name = \".\" + name + \".\"\n",
    "    # zip() is good here because it stops creating sets once there are no more sets of 3 to make\n",
    "    for ch1, ch2, ch3 in zip(name, name[1:], name[2:]):\n",
    "        # chars.append(((ch1, ch2), ch3))\n",
    "        \n",
    "        input_chars = ch1 + ch2\n",
    "        x.append(btoi[input_chars])\n",
    "        \n",
    "        y.append(stoi[ch3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "e99157e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([114, 729])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create one-hot encodings for all the inputs\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# F.one_hot(torch.tensor([1]), num_classes=729)\n",
    "for inputs in x:\n",
    "    xenc = F.one_hot(torch.tensor(x), num_classes=729).float()\n",
    "\n",
    "xenc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "eb31f927-c2c2-4f6f-9ae0-6ac63211609d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# time to contruct the neural network\n",
    "\n",
    "# fix the generator\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "\n",
    "# create the weights with randoms -> it should have 729, 27 so that it can give you a probability distribution for each of the 27 outputs. \n",
    "W = torch.randn((729, 27), generator=g, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "58c095ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here is the output if it is a 729, 1 random int matrix -> output is no_of_examples * 1, which does not really help you to estimate any probabilities\n",
    "W_wrong = torch.rand((729, 1), generator=g)\n",
    "wrong_output = xenc @ W_wrong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "4390bf94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([114, 27])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the predictions - because these output values have -ves in them, they can be considered to be log-counts\n",
    "logits = xenc @ W\n",
    "#  P.shape (13, 27)\n",
    "\n",
    "# remove the negatives\n",
    "P = logits.exp()\n",
    "\n",
    "# normalize the weights\n",
    "sum = P.sum(dim=1, keepdim=True)\n",
    "# sum.shape (13, 1)\n",
    "\n",
    "probs = P / sum\n",
    "\n",
    "probs.shape\n",
    "\n",
    "# verify that each row has a probability total of 1\n",
    "# probs[0, :].sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "4c91f19c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a reverse example that maps the index to the string\n",
    "itos = {i:s for s, i in stoi.items()}\n",
    "\n",
    "# make a reverse index that maps the index to the bigram\n",
    "itob = {i:b for b, i in btoi.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "46fef3eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.8278, grad_fn=<NegBackward0>)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# look at the probabilities for the desired outputs [14, 14, 2, 0, 13, 10, 23, 10, 2, 0, 23, 2, 0]\n",
    "# the 'loss' can be calculated using -log then take the mean\n",
    "number_of_elements = xenc.shape[0]\n",
    "loss = -probs[torch.arange(number_of_elements), y].log().mean()\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "02b0b47d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# try to start the process of gradient descent\n",
    "\n",
    "# set all the gradients to 0\n",
    "W.grad = None\n",
    "loss.backward()\n",
    "\n",
    "# W.grad\n",
    "\n",
    "W = -0.1 * W.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "fcc7139e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.2945)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the predictions - because these output values have -ves in them, they can be considered to be log-counts\n",
    "logits = xenc @ W\n",
    "#  P.shape (13, 27)\n",
    "\n",
    "# remove the negatives\n",
    "P = logits.exp()\n",
    "\n",
    "# normalize the weights\n",
    "sum = P.sum(dim=1, keepdim=True)\n",
    "# sum.shape (13, 1)\n",
    "\n",
    "probs = P / sum\n",
    "\n",
    "probs.shape\n",
    "\n",
    "number_of_elements = xenc.shape[0]\n",
    "loss = -probs[torch.arange(number_of_elements), y].log().mean()\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "0ad40d3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: 3.8278255462646484\n",
      "2: 3.6824936866760254\n",
      "3: 3.541057586669922\n",
      "4: 3.4043397903442383\n",
      "5: 3.2732536792755127\n",
      "6: 3.1485588550567627\n",
      "7: 3.030611276626587\n",
      "8: 2.919307231903076\n",
      "9: 2.814248561859131\n",
      "10: 2.714961051940918\n",
      "11: 2.62100887298584\n",
      "12: 2.532010555267334\n",
      "13: 2.4476168155670166\n",
      "14: 2.367496967315674\n",
      "15: 2.2913360595703125\n",
      "16: 2.2188422679901123\n",
      "17: 2.1497528553009033\n",
      "18: 2.083834648132324\n",
      "19: 2.0208845138549805\n",
      "20: 1.9607229232788086\n"
     ]
    }
   ],
   "source": [
    "# create one-hot encodings for all the inputs\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# F.one_hot(torch.tensor([1]), num_classes=729)\n",
    "for inputs in x:\n",
    "    xenc = F.one_hot(torch.tensor(x), num_classes=729).float()\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "W = torch.randn((729, 27), generator=g, requires_grad=True)\n",
    "\n",
    "for i in range(20):\n",
    "    logits = xenc @ W\n",
    "    p = logits.exp()\n",
    "    probs = p / p.sum(1, keepdims=True)\n",
    "\n",
    "    # collect the probabilities of the desired elements\n",
    "    loss = -probs[torch.arange(xenc.shape[0]), y].log().mean()\n",
    "    # loss.requires_grad = True\n",
    "    print(f\"{i + 1}: {loss}\")\n",
    "\n",
    "    # backpropagation\n",
    "    W.grad = None\n",
    "    loss.backward()\n",
    "\n",
    "    # -ve since you wanna decrease loss \n",
    "    # should be += rather than = , since you are altering the gradient, rather than setting it\n",
    "    W.data += -10 * W.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "d747d63e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jgmxzdfzjglkurxycczkwyhhmvlzimjtnagnrlkfdkzka.\n",
      "zug.\n",
      "chamzcpbbpwkhrggitmj.\n",
      "fibzmmqmkxujgfmtmdofekjeyktgscdgu.\n",
      "inkgvnrnfrqtbspmhwcjdewvtahlvsuqysfxxblgjxlhgfiwuidwnnjgpfdnipkezktsdesu.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "random.seed(2147483647)\n",
    "\n",
    "# try sampling from the trigram model lol\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "\n",
    "for i in range(5):\n",
    "  # random start, this is not right alr welps\n",
    "  ix = random.randint(0, 26)\n",
    "  prev_two = itob[ix]\n",
    "  out = [prev_two[1]]\n",
    "\n",
    "  while True:\n",
    "    xenc = F.one_hot(torch.tensor([ix]), num_classes=729).float()\n",
    "    logits = xenc @ W # predict log-counts\n",
    "    counts = logits.exp() # counts, equivalent to N\n",
    "    p = counts / counts.sum(1, keepdims=True) # probabilities for next character\n",
    "    \n",
    "\n",
    "    output_index = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()\n",
    "    out.append(itos[output_index])\n",
    "\n",
    "    ix = btoi[out[-2] + out[-1]]\n",
    "    if ix % 27 == 0:\n",
    "      break\n",
    "  print(''.join(out))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (torch)",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

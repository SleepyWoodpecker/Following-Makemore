{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt # for making figures\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32033\n",
      "15\n",
      "['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia']\n"
     ]
    }
   ],
   "source": [
    "# read in all the words\n",
    "words = open('../names.txt', 'r').read().splitlines()\n",
    "print(len(words))\n",
    "print(max(len(w) for w in words))\n",
    "print(words[:8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}\n",
      "27\n"
     ]
    }
   ],
   "source": [
    "# build the vocabulary of characters and mappings to/from integers\n",
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "itos = {i:s for s,i in stoi.items()}\n",
    "vocab_size = len(itos)\n",
    "print(itos)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([182625, 3]) torch.Size([182625])\n",
      "torch.Size([22655, 3]) torch.Size([22655])\n",
      "torch.Size([22866, 3]) torch.Size([22866])\n"
     ]
    }
   ],
   "source": [
    "# build the dataset\n",
    "block_size = 3 # context length: how many characters do we take to predict the next one?\n",
    "\n",
    "def build_dataset(words):  \n",
    "  X, Y = [], []\n",
    "  \n",
    "  for w in words:\n",
    "    context = [0] * block_size\n",
    "    for ch in w + '.':\n",
    "      ix = stoi[ch]\n",
    "      X.append(context)\n",
    "      Y.append(ix)\n",
    "      context = context[1:] + [ix] # crop and append\n",
    "\n",
    "  X = torch.tensor(X)\n",
    "  Y = torch.tensor(Y)\n",
    "  print(X.shape, Y.shape)\n",
    "  return X, Y\n",
    "\n",
    "import random\n",
    "random.seed(42)\n",
    "random.shuffle(words)\n",
    "n1 = int(0.8*len(words))\n",
    "n2 = int(0.9*len(words))\n",
    "\n",
    "Xtr,  Ytr  = build_dataset(words[:n1])     # 80%\n",
    "Xdev, Ydev = build_dataset(words[n1:n2])   # 10%\n",
    "Xte,  Yte  = build_dataset(words[n2:])     # 10%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utility function we will use later when comparing manual gradients to PyTorch gradients\n",
    "def cmp(s, dt, t):\n",
    "  ex = torch.all(dt == t.grad).item()\n",
    "  app = torch.allclose(dt, t.grad)\n",
    "  maxdiff = (dt - t.grad).abs().max().item()\n",
    "  print(f'{s:15s} | exact: {str(ex):5s} | approximate: {str(app):5s} | maxdiff: {maxdiff}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4137\n"
     ]
    }
   ],
   "source": [
    "n_embd = 10 # the dimensionality of the character embedding vectors\n",
    "n_hidden = 64 # the number of neurons in the hidden layer of the MLP\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647) # for reproducibility\n",
    "C  = torch.randn((vocab_size, n_embd),            generator=g)\n",
    "# Layer 1\n",
    "W1 = torch.randn((n_embd * block_size, n_hidden), generator=g) * (5/3)/((n_embd * block_size)**0.5)\n",
    "b1 = torch.randn(n_hidden,                        generator=g) * 0.1 # using b1 just for fun, it's useless because of BN\n",
    "# Layer 2\n",
    "W2 = torch.randn((n_hidden, vocab_size),          generator=g) * 0.1\n",
    "b2 = torch.randn(vocab_size,                      generator=g) * 0.1\n",
    "# BatchNorm parameters\n",
    "bngain = torch.randn((1, n_hidden))*0.1 + 1.0\n",
    "bnbias = torch.randn((1, n_hidden))*0.1\n",
    "\n",
    "# Note: I am initializating many of these parameters in non-standard ways\n",
    "# because sometimes initializating with e.g. all zeros could mask an incorrect\n",
    "# implementation of the backward pass.\n",
    "\n",
    "parameters = [C, W1, b1, W2, b2, bngain, bnbias]\n",
    "print(sum(p.nelement() for p in parameters)) # number of parameters in total\n",
    "for p in parameters:\n",
    "  p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "n = batch_size # a shorter variable also, for convenience\n",
    "# construct a minibatch\n",
    "ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n",
    "Xb, Yb = Xtr[ix], Ytr[ix] # batch X,Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.3334, grad_fn=<NegBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# forward pass, \"chunkated\" into smaller steps that are possible to backward one at a time\n",
    "\n",
    "emb = C[Xb] # embed the characters into vectors\n",
    "embcat = emb.view(emb.shape[0], -1) # concatenate the vectors\n",
    "# Linear layer 1\n",
    "hprebn = embcat @ W1 + b1 # hidden layer pre-activation\n",
    "# BatchNorm layer\n",
    "bnmeani = 1/n*hprebn.sum(0, keepdim=True)\n",
    "bndiff = hprebn - bnmeani\n",
    "bndiff2 = bndiff**2\n",
    "bnvar = 1/(n-1)*(bndiff2).sum(0, keepdim=True) # note: Bessel's correction (dividing by n-1, not n)\n",
    "bnvar_inv = (bnvar + 1e-5)**-0.5\n",
    "bnraw = bndiff * bnvar_inv\n",
    "hpreact = bngain * bnraw + bnbias\n",
    "# Non-linearity\n",
    "h = torch.tanh(hpreact) # hidden layer\n",
    "# Linear layer 2\n",
    "logits = h @ W2 + b2 # output layer\n",
    "# cross entropy loss (same as F.cross_entropy(logits, Yb))\n",
    "logit_maxes = logits.max(1, keepdim=True).values\n",
    "norm_logits = logits - logit_maxes # subtract max for numerical stability\n",
    "counts = norm_logits.exp()\n",
    "counts_sum = counts.sum(1, keepdims=True)\n",
    "counts_sum_inv = counts_sum**-1 # if I use (1.0 / counts_sum) instead then I can't get backprop to be bit exact...\n",
    "probs = counts * counts_sum_inv\n",
    "logprobs = probs.log()\n",
    "loss = -logprobs[range(n), Yb].mean()\n",
    "\n",
    "# PyTorch backward pass\n",
    "for p in parameters:\n",
    "  p.grad = None\n",
    "for t in [logprobs, probs, counts, counts_sum, counts_sum_inv, # afaik there is no cleaner way\n",
    "          norm_logits, logit_maxes, logits, h, hpreact, bnraw,\n",
    "         bnvar_inv, bnvar, bndiff2, bndiff, hprebn, bnmeani,\n",
    "         embcat, emb]:\n",
    "  t.retain_grad()\n",
    "loss.backward()\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems like the most important thing to figure out is:\n",
    "1) What is happening at the tensor level\n",
    "2) Preserve the shpae of the variable, was there any kind of operation that reduced the dimensionality of the tensor? If so, must take into account that broadcasting has occurred.\n",
    "3) Additionally, should also take into account the presence of the variable in other parts of the calculation\n",
    "4) Shape of the derivative should be the same as the initial value\n",
    "5) I guess if you are ever in doubt, you can take a smaller subset of the data to visualise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logprobs        | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "probs           | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "counts_sum_inv  | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "counts_sum      | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "counts          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "norm_logits     | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "logit_maxes     | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "logits          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "h               | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "W2              | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "b2              | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "hpreact         | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bngain          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bnbias          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bnraw           | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bnvar_inv       | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bnvar           | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bndiff2         | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bndiff          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bnmeani         | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "hprebn          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "embcat          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "W1              | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "b1              | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "emb             | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "C               | exact: True  | approximate: True  | maxdiff: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Exercise 1: backprop through the whole thing manually, \n",
    "# backpropagating through exactly all of the variables \n",
    "# as they are defined in the forward pass above, one by one\n",
    "\n",
    "# this is a function that lets you create 0s in the shape of the another tensor\n",
    "dlogprobs = torch.zeros_like(logprobs)\n",
    "\n",
    "# only these values have been indexed. Intuitively, the rest do not have a gradient, since they do not impact the loss\n",
    "dlogprobs[range(n), Yb] = -1/n # (32, 32)\n",
    "cmp('logprobs', dlogprobs, logprobs)\n",
    "\n",
    "dprobs = probs**-1 * dlogprobs\n",
    "cmp('probs', dprobs, probs)\n",
    "\n",
    "# the derivative is done like this because of tensor broadcasting. \n",
    "# keepdim = True here because you want to preserve the original shape of counts_sum_inv\n",
    "# because the counts_sum_inv is used multiple times during the forward pass, you want to take its sum across all the elements it has been boradcasted to\n",
    "dcounts_sum_inv = (counts * dprobs).sum(1, keepdim=True)\n",
    "cmp('counts_sum_inv', dcounts_sum_inv, counts_sum_inv)\n",
    "\n",
    "dcounts_sum = -(counts_sum)**-2 * dcounts_sum_inv\n",
    "cmp('counts_sum', dcounts_sum, counts_sum)\n",
    "\n",
    "# since it is just summing, derivative with respect to each individual count is always 1 -> kind of like a passthrough?\n",
    "# but I think the fact that counts being used more than once in the calculation of loss is where the complication arises\n",
    "\n",
    "dcounts = torch.ones_like(counts) * dcounts_sum + counts_sum_inv * dprobs\n",
    "cmp('counts', dcounts, counts)\n",
    "\n",
    "dnorm_logits = norm_logits.exp() * dcounts\n",
    "cmp('norm_logits', dnorm_logits, norm_logits)\n",
    "\n",
    "# since this is a regular subtraction equation, I would think that the value of the derivative is just 1\n",
    "# guess not -> the shape of logit_maxes is actually (32, 1), so there was broadcasting occurring to after getting the max value of logits \n",
    "# similarly here, because the column of logit_maxes was copied multiple times, you would also want to sum it across the column dimension\n",
    "dlogit_maxes = (-1 * dnorm_logits).sum(1, keepdim=True)\n",
    "cmp('logit_maxes', dlogit_maxes, logit_maxes)\n",
    "\n",
    "# the max operation, since it just takes a value, should have a gradient of 1 only on the logits where the maximum value came from\n",
    "# This was my mistake, but I guess using the keepdim=False flag here was important to ensure that the indexing was done for only 32 elementse\n",
    "# this could also have been done without any issues with a one_hot encoding\n",
    "dlogit_intermediate = torch.zeros_like(logits)\n",
    "dlogit_intermediate[torch.arange(logits.shape[0]), logits.max(1).indices] += 1\n",
    "dlogits = 1 * dnorm_logits + dlogit_intermediate * dlogit_maxes\n",
    "cmp('logits', dlogits, logits)\n",
    "\n",
    "# this has something to do with how the derivative of a tensor is derived\n",
    "# .T means the transpose of a matrix\n",
    "dh = dlogits @ W2.T\n",
    "cmp('h', dh, h)\n",
    "\n",
    "dW2 = h.T @ dlogits\n",
    "cmp('W2', dW2, W2)\n",
    "\n",
    "db2 = dlogits.sum(0)\n",
    "cmp('b2', db2, b2)\n",
    "\n",
    "dhpreact = (1 - h**2) * dh\n",
    "cmp('hpreact', dhpreact, hpreact)\n",
    "\n",
    "# dbngain/dhpreact = bnraw\n",
    "dbngain = (bnraw * dhpreact).sum(0, keepdim=True)\n",
    "cmp('bngain', dbngain, bngain)\n",
    "\n",
    "# dbnbias shape = 1, 64\n",
    "dbnbias = dhpreact.sum(0, keepdim=True)\n",
    "cmp('bnbias', dbnbias, bnbias)\n",
    "\n",
    "# desired shape is 32, 64\n",
    "dbnraw = bngain * dhpreact\n",
    "cmp('bnraw', dbnraw, bnraw)\n",
    "\n",
    "# the desired shape is (1, 64) -> this means that broadcasting occured, and that the derivative should be the sum\n",
    "dbnvar_inv = (bndiff * dbnraw).sum(0, keepdim=True)\n",
    "cmp('bnvar_inv', dbnvar_inv, bnvar_inv)\n",
    "\n",
    "dbnvar = (-0.5 * (bnvar + 1e-5)**-1.5) * dbnvar_inv\n",
    "cmp('bnvar', dbnvar, bnvar)\n",
    "\n",
    "# when you perform a .sum(), all that you are doing is addition. so derivative wrt to every element should be 1\n",
    "bndiff_intermediate = torch.ones_like(bndiff2) * 1/(n-1)\n",
    "dbndiff2 = bndiff_intermediate * dbnvar\n",
    "cmp('bndiff2', dbndiff2, bndiff2)\n",
    "\n",
    "dbndiff = 2 * bndiff * dbndiff2 + bnvar_inv * dbnraw\n",
    "cmp('bndiff', dbndiff, bndiff)\n",
    "\n",
    "# based on the shapes, bnmeani was broadcasted, so this means that it should be added\n",
    "dbnmeani = -dbndiff.sum(0, keepdim=True)\n",
    "cmp('bnmeani', dbnmeani, bnmeani)\n",
    "\n",
    "dhprebn_intermediate = 1/(n) * dbnmeani\n",
    "dhprebn = dbndiff + dhprebn_intermediate\n",
    "cmp('hprebn', dhprebn, hprebn)\n",
    "\n",
    "dembcat = dhprebn @ W1.T\n",
    "cmp('embcat', dembcat, embcat)\n",
    "\n",
    "dW1 = embcat.T @ dhprebn\n",
    "cmp('W1', dW1, W1)\n",
    "\n",
    "db1 = dhprebn.sum(0, keepdim=True)\n",
    "cmp('b1', db1, b1)\n",
    "\n",
    "# I would think this is just reorganizing the view?\n",
    "demb = dembcat.view(batch_size, block_size, n_embd) \n",
    "cmp('emb', demb, emb)\n",
    "\n",
    "# for C, only the characters that actually got selected in this batch should be adjusted\n",
    "# something that you missed out here is that you have to update the indexes (at a single character level)\n",
    "dC = torch.zeros_like(C)\n",
    "\n",
    "# iterate through the selected character embeddings and update them\n",
    "# this is abit scuffed, will have to revisit lol\n",
    "for i in range(Xb.shape[0]):\n",
    "    for j in range(Xb.shape[1]):\n",
    "        index_of_character_to_update = Xb[i, j]\n",
    "        # you can do this here because the embeddings are created with Xb\n",
    "        dC[index_of_character_to_update] += demb[i, j]\n",
    "\n",
    "cmp('C', dC, C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.333357334136963 diff: -4.76837158203125e-07\n"
     ]
    }
   ],
   "source": [
    "# Exercise 2: backprop through cross_entropy but all in one go\n",
    "# to complete this challenge look at the mathematical expression of the loss,\n",
    "# take the derivative, simplify the expression, and just write it out\n",
    "\n",
    "# forward pass\n",
    "\n",
    "# before:\n",
    "# logit_maxes = logits.max(1, keepdim=True).values\n",
    "# norm_logits = logits - logit_maxes # subtract max for numerical stability\n",
    "# counts = norm_logits.exp()\n",
    "# counts_sum = counts.sum(1, keepdims=True)\n",
    "# counts_sum_inv = counts_sum**-1 # if I use (1.0 / counts_sum) instead then I can't get backprop to be bit exact...\n",
    "# probs = counts * counts_sum_inv\n",
    "# logprobs = probs.log()\n",
    "# loss = -logprobs[range(n), Yb].mean()\n",
    "\n",
    "# now:\n",
    "loss_fast = F.cross_entropy(logits, Yb)\n",
    "print(loss_fast.item(), 'diff:', (loss_fast - loss).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 27])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "softmaxed = F.softmax(logits, dim=1)\n",
    "softmaxed[0, :].sum()\n",
    "softmaxed[range(n), Yb] = softmaxed[range(n), Yb] - 1\n",
    "softmaxed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 27])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits          | exact: False | approximate: False | maxdiff: 0.9558247923851013\n"
     ]
    }
   ],
   "source": [
    "# backward pass\n",
    "\n",
    "# -----------------\n",
    "# YOUR CODE HERE :)\n",
    "\n",
    "# these are the logits that were not observed in the output\n",
    "dlogits = torch.softmax(logits, dim=1) * 1/32\n",
    "# these are the logits that were observed at the output\n",
    "dlogits[range(n), Yb] = -(1 - dlogits[range(n), Yb])\n",
    "# dlogits = None # TODO. my solution is 3 lines\n",
    "# -----------------\n",
    "\n",
    "cmp('logits', dlogits, logits) # I can only get approximate to be true, my maxdiff is 6e-9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0707,  0.0884,  0.0178,  0.0511,  0.0190,  0.0786,  0.0235,  0.0364,\n",
       "         -0.9797,  0.0304,  0.0362,  0.0356,  0.0403,  0.0316,  0.0373,  0.0137,\n",
       "          0.0094,  0.0186,  0.0160,  0.0542,  0.0520,  0.0227,  0.0277,  0.0672,\n",
       "          0.0537,  0.0265,  0.0213],\n",
       "        [ 0.0527,  0.0574,  0.0850,  0.0549,  0.0343,  0.0327,  0.0200,  0.0458,\n",
       "          0.0239,  0.0247,  0.0472,  0.0453,  0.0491,  0.0300, -0.9517,  0.0379,\n",
       "          0.0263,  0.0176,  0.0201,  0.0385,  0.0192,  0.0246,  0.0157,  0.0626,\n",
       "          0.0224,  0.0368,  0.0267],\n",
       "        [ 0.0187,  0.0234,  0.0147,  0.0133,  0.0227,  0.0394,  0.0564,  0.0603,\n",
       "          0.0699,  0.0300,  0.0194,  0.0334,  0.0476,  0.0505,  0.0238, -0.9763,\n",
       "          0.0144,  0.0327,  0.0276,  0.1077,  0.0677,  0.0395,  0.0442,  0.0369,\n",
       "          0.0343,  0.0206,  0.0273],\n",
       "        [ 0.0310,  0.0261,  0.0396,  0.0535,  0.0576,  0.0265,  0.0456,  0.0419,\n",
       "          0.0549,  0.0190,  0.0334,  0.0247,  0.0364,  0.0460,  0.0636,  0.0653,\n",
       "          0.0245,  0.0245,  0.0171,  0.0498,  0.0182,  0.0247, -0.9588,  0.0192,\n",
       "          0.0252,  0.0458,  0.0445],\n",
       "        [-0.9840,  0.0162,  0.0248,  0.0249,  0.0184,  0.0393,  0.0342,  0.0180,\n",
       "          0.0645,  0.0377,  0.0300,  0.0142,  0.0441,  0.0265,  0.0504,  0.1605,\n",
       "          0.0198,  0.0508,  0.0453,  0.0593,  0.0430,  0.0171,  0.0251,  0.0137,\n",
       "          0.0401,  0.0234,  0.0426],\n",
       "        [ 0.0350,  0.0363,  0.0513,  0.0671,  0.0661,  0.0254,  0.0297,  0.0510,\n",
       "          0.0317,  0.0164,  0.0400,  0.0177,  0.0298,  0.0462,  0.0669,  0.0495,\n",
       "          0.0165,  0.0210,  0.0103, -0.9712,  0.0316,  0.0305,  0.0468,  0.0173,\n",
       "          0.0170,  0.0761,  0.0440],\n",
       "        [ 0.0542,  0.0114,  0.0636,  0.0158,  0.0293,  0.0200,  0.0615,  0.0530,\n",
       "          0.0537, -0.9578,  0.0177,  0.0333,  0.0305,  0.0183,  0.0168,  0.0670,\n",
       "          0.0793,  0.0443,  0.0103,  0.0335,  0.0290,  0.0540,  0.0487,  0.0193,\n",
       "          0.0383,  0.0241,  0.0310],\n",
       "        [ 0.0449,  0.0307,  0.0235,  0.1179,  0.0286,  0.0492,  0.0300,  0.0831,\n",
       "          0.0405,  0.0087,  0.0277,  0.0153,  0.0205,  0.0596, -0.9813,  0.0136,\n",
       "          0.0141,  0.0128,  0.0145,  0.0895,  0.0356,  0.0489,  0.0276,  0.0335,\n",
       "          0.0210,  0.0545,  0.0355],\n",
       "        [ 0.0498,  0.0298,  0.0175,  0.0360,  0.0106, -0.9602,  0.0544,  0.0866,\n",
       "          0.0322,  0.0198,  0.0269,  0.0324,  0.0393,  0.0239,  0.0119,  0.0184,\n",
       "          0.0206,  0.0376,  0.0214,  0.0882,  0.0384,  0.0910,  0.0434,  0.0407,\n",
       "          0.0383,  0.0157,  0.0353],\n",
       "        [ 0.0252, -0.9858,  0.0276,  0.0194,  0.0303,  0.0348,  0.0323,  0.0293,\n",
       "          0.1046,  0.0586,  0.0120,  0.0346,  0.0433,  0.0448,  0.0248,  0.0250,\n",
       "          0.0622,  0.0434,  0.0411,  0.0242,  0.0705,  0.0286,  0.0677,  0.0179,\n",
       "          0.0284,  0.0370,  0.0183],\n",
       "        [ 0.0145,  0.0074,  0.0349,  0.0439,  0.0345,  0.0358,  0.0193,  0.0062,\n",
       "          0.0428,  0.0455,  0.0231,  0.0174,  0.0636,  0.0306,  0.0425,  0.1351,\n",
       "          0.0551,  0.0590,  0.0776,  0.0349, -0.9546,  0.0106,  0.0134,  0.0210,\n",
       "          0.0356,  0.0282,  0.0222],\n",
       "        [ 0.0223,  0.0290,  0.0219, -0.8115,  0.0118,  0.0552,  0.0689,  0.0451,\n",
       "          0.0497,  0.0294,  0.0135,  0.0188,  0.0237,  0.0289,  0.0069,  0.0067,\n",
       "          0.0217,  0.0345,  0.0156,  0.0567,  0.0521,  0.0411,  0.0218,  0.0337,\n",
       "          0.0420,  0.0446,  0.0157],\n",
       "        [ 0.0172,  0.0133,  0.0450,  0.0150,  0.0272,  0.0157,  0.0757,  0.0366,\n",
       "         -0.9350,  0.0526,  0.0160,  0.0361,  0.0428,  0.0321,  0.0164,  0.0381,\n",
       "          0.0727,  0.0439,  0.0416,  0.0226,  0.0481,  0.0416,  0.1001,  0.0102,\n",
       "          0.0212,  0.0261,  0.0273],\n",
       "        [ 0.0213,  0.0269,  0.0493,  0.0210,  0.0174,  0.0670,  0.0270,  0.0149,\n",
       "          0.0356,  0.0470,  0.0283,  0.0305,  0.0772,  0.0384, -0.9437,  0.0415,\n",
       "          0.0302,  0.0350,  0.0811,  0.0147,  0.0645,  0.0170,  0.0193,  0.0323,\n",
       "          0.0314,  0.0396,  0.0351],\n",
       "        [ 0.0445,  0.0257,  0.0271,  0.0516,  0.0180,  0.0336,  0.0305,  0.0554,\n",
       "          0.0607,  0.0506,  0.0292,  0.0433, -0.9458,  0.0419,  0.0300,  0.0230,\n",
       "          0.0233,  0.0525,  0.0282,  0.0468,  0.0546,  0.0514,  0.0265,  0.0334,\n",
       "          0.0203,  0.0205,  0.0233],\n",
       "        [-0.9607,  0.0548,  0.0298,  0.0190,  0.0198,  0.0765,  0.0311,  0.0226,\n",
       "          0.0355,  0.0556,  0.0341,  0.0455,  0.0681,  0.0170,  0.0316,  0.0160,\n",
       "          0.0416,  0.0209,  0.0581,  0.0174,  0.0394,  0.0277,  0.0358,  0.0616,\n",
       "          0.0504,  0.0301,  0.0207],\n",
       "        [ 0.0267,  0.0246,  0.0796,  0.0465,  0.0610,  0.0501,  0.0168,  0.0271,\n",
       "          0.0222,  0.0194,  0.0444, -0.9785,  0.0455,  0.0597,  0.0706,  0.0622,\n",
       "          0.0156,  0.0174,  0.0246,  0.0463,  0.0277,  0.0167,  0.0100,  0.0441,\n",
       "          0.0223,  0.0434,  0.0539],\n",
       "        [-0.9482,  0.0790,  0.0481,  0.0555,  0.0393,  0.0330,  0.0214,  0.0725,\n",
       "          0.0280,  0.0176,  0.0653,  0.0149,  0.0243,  0.0205,  0.0438,  0.0280,\n",
       "          0.0146,  0.0297,  0.0169,  0.0517,  0.0223,  0.0477,  0.0276,  0.0181,\n",
       "          0.0225,  0.0428,  0.0633],\n",
       "        [ 0.0172,  0.0133,  0.0450,  0.0150,  0.0272,  0.0157,  0.0757,  0.0366,\n",
       "          0.0650,  0.0526,  0.0160,  0.0361,  0.0428,  0.0321,  0.0164,  0.0381,\n",
       "          0.0727,  0.0439,  0.0416,  0.0226,  0.0481,  0.0416,  0.1001,  0.0102,\n",
       "          0.0212,  0.0261, -0.9727],\n",
       "        [ 0.0235,  0.0218,  0.0340,  0.0165,  0.0370,  0.0311,  0.0656,  0.0485,\n",
       "          0.0408, -0.9644,  0.0286,  0.0450,  0.0632,  0.0571,  0.0156,  0.0231,\n",
       "          0.0298,  0.0203,  0.0293,  0.0792,  0.0525,  0.0369,  0.0417,  0.0458,\n",
       "          0.0287,  0.0230,  0.0260],\n",
       "        [ 0.0533,  0.0140,  0.0180,  0.0388,  0.0313,  0.0348,  0.0277,  0.0398,\n",
       "          0.0602,  0.0219,  0.0584,  0.0358,  0.0638,  0.0236,  0.0475,  0.0713,\n",
       "          0.0169,  0.0212,  0.0350,  0.0537,  0.0194,  0.0321,  0.0258,  0.0238,\n",
       "          0.0344, -0.9421,  0.0393],\n",
       "        [-0.9500,  0.0706,  0.0471,  0.0518,  0.0240,  0.0610,  0.0309,  0.0448,\n",
       "          0.0284,  0.0165,  0.0474,  0.0270,  0.0262,  0.0334,  0.0405,  0.0423,\n",
       "          0.0193,  0.0352,  0.0199,  0.0927,  0.0134,  0.0362,  0.0157,  0.0291,\n",
       "          0.0317,  0.0297,  0.0350],\n",
       "        [ 0.0172, -0.9867,  0.0450,  0.0150,  0.0272,  0.0157,  0.0757,  0.0366,\n",
       "          0.0650,  0.0526,  0.0160,  0.0361,  0.0428,  0.0321,  0.0164,  0.0381,\n",
       "          0.0727,  0.0439,  0.0416,  0.0226,  0.0481,  0.0416,  0.1001,  0.0102,\n",
       "          0.0212,  0.0261,  0.0273],\n",
       "        [ 0.0442, -0.9822,  0.0235,  0.0131,  0.0578,  0.0248,  0.0646,  0.0320,\n",
       "          0.0272,  0.0399,  0.0276,  0.0367,  0.0390,  0.0344,  0.0373,  0.0443,\n",
       "          0.0750,  0.0213,  0.0364,  0.0160,  0.0190,  0.0244,  0.1250,  0.0165,\n",
       "          0.0545,  0.0313,  0.0164],\n",
       "        [ 0.0340,  0.0808,  0.0231,  0.0499,  0.0270,  0.0608,  0.0240, -0.9695,\n",
       "          0.0311,  0.0289,  0.0932,  0.0171,  0.0309,  0.0124,  0.0395,  0.0186,\n",
       "          0.0174,  0.0154,  0.0117,  0.0276,  0.0139,  0.0259,  0.0116,  0.1494,\n",
       "          0.0461,  0.0261,  0.0530],\n",
       "        [ 0.0648,  0.0260,  0.0145,  0.0210,  0.0336,  0.0755,  0.0441,  0.0323,\n",
       "          0.0319,  0.0348,  0.0283,  0.0511,  0.0568,  0.0369,  0.0218,  0.0118,\n",
       "          0.0347,  0.0220, -0.9489,  0.0136,  0.0268,  0.0240,  0.0737,  0.0499,\n",
       "          0.0623,  0.0454,  0.0110],\n",
       "        [ 0.0399,  0.0231,  0.0197,  0.0106,  0.0464,  0.0210,  0.0872,  0.0387,\n",
       "          0.0327, -0.9505,  0.0157,  0.0435,  0.0634,  0.0301,  0.0265,  0.0171,\n",
       "          0.0592,  0.0267,  0.0292,  0.0333,  0.0227,  0.0382,  0.1352,  0.0174,\n",
       "          0.0368,  0.0264,  0.0100],\n",
       "        [ 0.0245,  0.0268,  0.0438, -0.9786,  0.0358,  0.0225,  0.0353,  0.0275,\n",
       "          0.0468,  0.0519,  0.0185,  0.0267,  0.0429,  0.0369,  0.0483,  0.0593,\n",
       "          0.0481,  0.0478,  0.0508,  0.0163,  0.0981,  0.0165,  0.0486,  0.0112,\n",
       "          0.0165,  0.0340,  0.0431],\n",
       "        [ 0.0659,  0.0372,  0.0144,  0.0214,  0.0363, -0.9399,  0.0425,  0.0375,\n",
       "          0.0124,  0.0259,  0.0392,  0.0271,  0.0268,  0.0216,  0.0293,  0.0287,\n",
       "          0.0791,  0.0257,  0.0626,  0.0251,  0.0347,  0.0354,  0.0487,  0.0582,\n",
       "          0.0736,  0.0125,  0.0180],\n",
       "        [ 0.0200,  0.0169,  0.0294,  0.0327,  0.0494,  0.0202,  0.0631,  0.0337,\n",
       "          0.0783, -0.9682,  0.0254,  0.0338,  0.0352,  0.0404,  0.0353,  0.0638,\n",
       "          0.0304,  0.0262,  0.0372,  0.0277,  0.0432,  0.0292,  0.0622,  0.0127,\n",
       "          0.0323,  0.0522,  0.0372],\n",
       "        [-0.9623,  0.0511,  0.0227,  0.0645,  0.0455,  0.0491,  0.0337,  0.0314,\n",
       "          0.0326,  0.0203,  0.0348,  0.0228,  0.0189,  0.0271,  0.0119,  0.0311,\n",
       "          0.0344,  0.0277,  0.0453,  0.0513,  0.0542,  0.0291,  0.0222,  0.0755,\n",
       "          0.0774,  0.0211,  0.0266],\n",
       "        [ 0.0454,  0.0229,  0.0612,  0.0196,  0.0749,  0.0275,  0.0170,  0.0251,\n",
       "          0.0176,  0.0536,  0.0249,  0.0304,  0.0362,  0.0837,  0.0423,  0.0362,\n",
       "          0.0500,  0.0426, -0.9592,  0.0141,  0.0607,  0.0106,  0.0663,  0.0217,\n",
       "          0.0172,  0.0379,  0.0196]], grad_fn=<IndexPutBackward0>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dlogits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3: backprop through batchnorm but all in one go\n",
    "# to complete this challenge look at the mathematical expression of the output of batchnorm,\n",
    "# take the derivative w.r.t. its input, simplify the expression, and just write it out\n",
    "# BatchNorm paper: https://arxiv.org/abs/1502.03167\n",
    "\n",
    "# forward pass\n",
    "\n",
    "# before:\n",
    "# bnmeani = 1/n*hprebn.sum(0, keepdim=True)\n",
    "# bndiff = hprebn - bnmeani\n",
    "# bndiff2 = bndiff**2\n",
    "# bnvar = 1/(n-1)*(bndiff2).sum(0, keepdim=True) # note: Bessel's correction (dividing by n-1, not n)\n",
    "# bnvar_inv = (bnvar + 1e-5)**-0.5\n",
    "# bnraw = bndiff * bnvar_inv\n",
    "# hpreact = bngain * bnraw + bnbias\n",
    "\n",
    "# now:\n",
    "hpreact_fast = bngain * (hprebn - hprebn.mean(0, keepdim=True)) / torch.sqrt(hprebn.var(0, keepdim=True, unbiased=True) + 1e-5) + bnbias\n",
    "print('max diff:', (hpreact_fast - hpreact).abs().max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# backward pass\n",
    "\n",
    "# before we had:\n",
    "# dbnraw = bngain * dhpreact\n",
    "# dbndiff = bnvar_inv * dbnraw\n",
    "# dbnvar_inv = (bndiff * dbnraw).sum(0, keepdim=True)\n",
    "# dbnvar = (-0.5*(bnvar + 1e-5)**-1.5) * dbnvar_inv\n",
    "# dbndiff2 = (1.0/(n-1))*torch.ones_like(bndiff2) * dbnvar\n",
    "# dbndiff += (2*bndiff) * dbndiff2\n",
    "# dhprebn = dbndiff.clone()\n",
    "# dbnmeani = (-dbndiff).sum(0)\n",
    "# dhprebn += 1.0/n * (torch.ones_like(hprebn) * dbnmeani)\n",
    "\n",
    "# calculate dhprebn given dhpreact (i.e. backprop through the batchnorm)\n",
    "# (you'll also need to use some of the variables from the forward pass up above)\n",
    "\n",
    "# -----------------\n",
    "# YOUR CODE HERE :)\n",
    "dhprebn = None # TODO. my solution is 1 (long) line\n",
    "# -----------------\n",
    "\n",
    "cmp('hprebn', dhprebn, hprebn) # I can only get approximate to be true, my maxdiff is 9e-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 4: putting it all together!\n",
    "# Train the MLP neural net with your own backward pass\n",
    "\n",
    "# init\n",
    "n_embd = 10 # the dimensionality of the character embedding vectors\n",
    "n_hidden = 200 # the number of neurons in the hidden layer of the MLP\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647) # for reproducibility\n",
    "C  = torch.randn((vocab_size, n_embd),            generator=g)\n",
    "# Layer 1\n",
    "W1 = torch.randn((n_embd * block_size, n_hidden), generator=g) * (5/3)/((n_embd * block_size)**0.5)\n",
    "b1 = torch.randn(n_hidden,                        generator=g) * 0.1\n",
    "# Layer 2\n",
    "W2 = torch.randn((n_hidden, vocab_size),          generator=g) * 0.1\n",
    "b2 = torch.randn(vocab_size,                      generator=g) * 0.1\n",
    "# BatchNorm parameters\n",
    "bngain = torch.randn((1, n_hidden))*0.1 + 1.0\n",
    "bnbias = torch.randn((1, n_hidden))*0.1\n",
    "\n",
    "parameters = [C, W1, b1, W2, b2, bngain, bnbias]\n",
    "print(sum(p.nelement() for p in parameters)) # number of parameters in total\n",
    "for p in parameters:\n",
    "  p.requires_grad = True\n",
    "\n",
    "# same optimization as last time\n",
    "max_steps = 200000\n",
    "batch_size = 32\n",
    "n = batch_size # convenience\n",
    "lossi = []\n",
    "\n",
    "# use this context manager for efficiency once your backward pass is written (TODO)\n",
    "#with torch.no_grad():\n",
    "\n",
    "# kick off optimization\n",
    "for i in range(max_steps):\n",
    "\n",
    "  # minibatch construct\n",
    "  ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n",
    "  Xb, Yb = Xtr[ix], Ytr[ix] # batch X,Y\n",
    "\n",
    "  # forward pass\n",
    "  emb = C[Xb] # embed the characters into vectors\n",
    "  embcat = emb.view(emb.shape[0], -1) # concatenate the vectors\n",
    "  # Linear layer\n",
    "  hprebn = embcat @ W1 + b1 # hidden layer pre-activation\n",
    "  # BatchNorm layer\n",
    "  # -------------------------------------------------------------\n",
    "  bnmean = hprebn.mean(0, keepdim=True)\n",
    "  bnvar = hprebn.var(0, keepdim=True, unbiased=True)\n",
    "  bnvar_inv = (bnvar + 1e-5)**-0.5\n",
    "  bnraw = (hprebn - bnmean) * bnvar_inv\n",
    "  hpreact = bngain * bnraw + bnbias\n",
    "  # -------------------------------------------------------------\n",
    "  # Non-linearity\n",
    "  h = torch.tanh(hpreact) # hidden layer\n",
    "  logits = h @ W2 + b2 # output layer\n",
    "  loss = F.cross_entropy(logits, Yb) # loss function\n",
    "\n",
    "  # backward pass\n",
    "  for p in parameters:\n",
    "    p.grad = None\n",
    "  loss.backward() # use this for correctness comparisons, delete it later!\n",
    "\n",
    "  # manual backprop! #swole_doge_meme\n",
    "  # -----------------\n",
    "  # YOUR CODE HERE :)\n",
    "  dC, dW1, db1, dW2, db2, dbngain, dbnbias = None, None, None, None, None, None, None\n",
    "  grads = [dC, dW1, db1, dW2, db2, dbngain, dbnbias]\n",
    "  # -----------------\n",
    "\n",
    "  # update\n",
    "  lr = 0.1 if i < 100000 else 0.01 # step learning rate decay\n",
    "  for p, grad in zip(parameters, grads):\n",
    "    p.data += -lr * p.grad # old way of cheems doge (using PyTorch grad from .backward())\n",
    "    #p.data += -lr * grad # new way of swole doge TODO: enable\n",
    "\n",
    "  # track stats\n",
    "  if i % 10000 == 0: # print every once in a while\n",
    "    print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')\n",
    "  lossi.append(loss.log10().item())\n",
    "\n",
    "  if i >= 100: # TODO: delete early breaking when you're ready to train the full net\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# useful for checking your gradients\n",
    "# for p,g in zip(parameters, grads):\n",
    "#   cmp(str(tuple(p.shape)), g, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calibrate the batch norm at the end of training\n",
    "\n",
    "with torch.no_grad():\n",
    "  # pass the training set through\n",
    "  emb = C[Xtr]\n",
    "  embcat = emb.view(emb.shape[0], -1)\n",
    "  hpreact = embcat @ W1 + b1\n",
    "  # measure the mean/std over the entire training set\n",
    "  bnmean = hpreact.mean(0, keepdim=True)\n",
    "  bnvar = hpreact.var(0, keepdim=True, unbiased=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate train and val loss\n",
    "\n",
    "@torch.no_grad() # this decorator disables gradient tracking\n",
    "def split_loss(split):\n",
    "  x,y = {\n",
    "    'train': (Xtr, Ytr),\n",
    "    'val': (Xdev, Ydev),\n",
    "    'test': (Xte, Yte),\n",
    "  }[split]\n",
    "  emb = C[x] # (N, block_size, n_embd)\n",
    "  embcat = emb.view(emb.shape[0], -1) # concat into (N, block_size * n_embd)\n",
    "  hpreact = embcat @ W1 + b1\n",
    "  hpreact = bngain * (hpreact - bnmean) * (bnvar + 1e-5)**-0.5 + bnbias\n",
    "  h = torch.tanh(hpreact) # (N, n_hidden)\n",
    "  logits = h @ W2 + b2 # (N, vocab_size)\n",
    "  loss = F.cross_entropy(logits, y)\n",
    "  print(split, loss.item())\n",
    "\n",
    "split_loss('train')\n",
    "split_loss('val')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gp-gpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
